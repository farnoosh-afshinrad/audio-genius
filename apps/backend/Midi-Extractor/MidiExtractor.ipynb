{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "665a6029-34a3-4a50-b94c-231868936a0c",
   "metadata": {},
   "source": [
    "## MidiExtractor\n",
    "#### Basic usage of the wave_to_midi function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b851506-f8c7-464a-bd6f-e282794bb5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luigi\\Desktop\\test-midi-extractor\\monophonic.py:126: RuntimeWarning: invalid value encountered in cast\n",
      "  f0_ = np.round(librosa.hz_to_midi(pitch - tuning)).astype(int)\n",
      "C:\\Users\\Luigi\\Desktop\\test-midi-extractor\\monophonic.py:151: RuntimeWarning: overflow encountered in scalar subtract\n",
      "  elif np.abs(j + midi_min - f0_[n_frame]) == 1:\n",
      "C:\\Users\\Luigi\\Desktop\\test-midi-extractor\\monophonic.py:323: FutureWarning: librosa.beat.tempo\n",
      "\tThis function was moved to 'librosa.feature.rhythm.tempo' in librosa version 0.10.0.\n",
      "\tThis alias will be removed in librosa version 1.0.\n",
      "  bpm = librosa.beat.tempo(y=audio_signal)[0]\n"
     ]
    }
   ],
   "source": [
    "'''from monophonic import wave_to_midi\n",
    "import librosa\n",
    "\n",
    "wav, Fs = librosa.load(\"7732.mp3\")\n",
    "midi = wave_to_midi(wav)\n",
    "with open(\"test.mid\", \"wb\") as output_file:\n",
    "    midi.writeFile(output_file)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76aa8665-27bf-4df2-a5ba-fbf6187d089c",
   "metadata": {},
   "source": [
    "#### Rewriting and adapting the previously used method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a869242e-2e5e-4ad3-bbcd-670189a6fc04",
   "metadata": {},
   "source": [
    "Create the MidiExtractor class (let's keep in mind that this works for monophonic detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b5dba2-5200-4600-b49e-c921175a76a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from monophonic import wave_to_midi\n",
    "import librosa\n",
    "\n",
    "wav, Fs = librosa.load(\"7732.mp3\")\n",
    "midi = wave_to_midi(wav)\n",
    "with open(\"test.mid\", \"wb\") as output_file:\n",
    "    midi.writeFile(output_file)'''\n",
    "\n",
    "import numpy as np\n",
    "import torchcrepe\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import midiutil\n",
    "import math\n",
    "import json\n",
    "\n",
    "class MidiExtractor:\n",
    "    def __init__(self):\n",
    "        self.noteMap = {\n",
    "            36: 'C2', 37: 'C#2', 38: 'D2', 39: 'D#2', 40: 'E2',\n",
    "            41: 'F2', 42: 'F#2', 43: 'G2', 44: 'G#2', 45: 'A2',\n",
    "            46: 'A#2', 47: 'B2', 48: 'C3', 49: 'C#3', 50: 'D3',\n",
    "            51: 'D#3', 52: 'E3', 53: 'F3', 54: 'F#3', 55: 'G3',\n",
    "            56: 'G#3', 57: 'A3', 58: 'A#3', 59: 'B3', 60: 'C4',\n",
    "            61: 'C#4', 62: 'D4', 63: 'D#4', 64: 'E4', 65: 'F4',\n",
    "            66: 'F#4', 67: 'G4', 68: 'G#4', 69: 'A4', 70: 'A#4',\n",
    "            71: 'B4', 72: 'C5', 73: 'C#5', 74: 'D5', 75: 'D#5',\n",
    "            76: 'E5', 77: 'F5', 78: 'F#5', 79: 'G5', 80: 'G#5',\n",
    "            81: 'A5', 82: 'A#5', 83: 'B5', 84: 'C6'\n",
    "        }\n",
    "        \n",
    "        self.noteMapHz = {}\n",
    "        self.originalPitch = None\n",
    "\n",
    "        keys = list(self.noteMap.keys())\n",
    "        \n",
    "        for k in keys:\n",
    "            self.noteMapHz[k] = librosa.note_to_hz(self.noteMap[k])\n",
    "\n",
    "        self.noteMapValues = list(self.noteMap.values())\n",
    "\n",
    "        self.midiMin = keys[0]\n",
    "        self.midiMax = keys[-1]\n",
    "\n",
    "    @staticmethod\n",
    "    def __rangeConversion(oldValue: float, oldRangeTuple: tuple[float, float], newRangeTuple: tuple[float, float]) -> float:\n",
    "        oldRange = (oldRangeTuple[1] - oldRangeTuple[0])\n",
    "        newRange = (newRangeTuple[1] - newRangeTuple[0])\n",
    "        return (((oldValue - oldRangeTuple[0]) * newRange) / oldRange) + newRangeTuple[0]\n",
    "        \n",
    "    def __transitionMatrix(self, pStayNote: float, pStaySilence: float) -> np.array:\n",
    "        nNotes = self.midiMax - self.midiMin + 1\n",
    "        pL = (1 - pStaySilence) / nNotes\n",
    "        pLL = (1 - pStayNote) / (nNotes + 1)\n",
    "\n",
    "        # Initialize the transition matrix\n",
    "        transMat = np.zeros((2 * nNotes + 1, 2 * nNotes + 1))\n",
    "        \n",
    "        # State 0 = silence\n",
    "        transMat[0, 0] = pStaySilence\n",
    "        for i in range(nNotes):\n",
    "            transMat[0, (i * 2) + 1] = pL\n",
    "\n",
    "        # Odd states = onsets\n",
    "        for i in range(nNotes):\n",
    "            transMat[(i * 2) + 1, (i * 2) + 2] = 1\n",
    "\n",
    "        # Even states = sustains\n",
    "        for i in range(nNotes):\n",
    "            transMat[(i * 2) + 2, 0] = pLL\n",
    "            transMat[(i * 2) + 2, (i * 2) + 2] = pStayNote\n",
    "            for j in range(nNotes):\n",
    "                transMat[(i * 2) + 2, (j * 2) + 1] = pLL\n",
    "\n",
    "        return transMat\n",
    "\n",
    "    def __detectVocalOnsets(self, frequencies: np.array, t: int = 1) -> np.array:\n",
    "        threshold = librosa.note_to_hz(self.noteMapValues[-1])\n",
    "        \n",
    "        maximum = 0\n",
    "        modF0 = np.nan_to_num(frequencies, copy=True)\n",
    "        for i in range(len(frequencies)):\n",
    "            if modF0[i] > maximum:\n",
    "                maximum = modF0[i]\n",
    "            modF0[i] = (modF0[i] * threshold) / maximum\n",
    "\n",
    "        # Differentiate\n",
    "        slopes = np.diff(modF0)\n",
    "\n",
    "        # Calculate summation of slopes (detecting sign changes)\n",
    "        s = 0\n",
    "        sameSlopeDir = np.zeros(len(slopes))\n",
    "        followingSlopes = np.zeros(len(slopes))\n",
    "        for i in range(len(slopes)):\n",
    "            s = slopes[i]\n",
    "            j = i + 1\n",
    "\n",
    "            while (j < len(slopes)) and ((slopes[i] > 0 and slopes[j] > 0) or (slopes[i] < 0 and slopes[j] < 0)):\n",
    "                s = s + slopes[j]\n",
    "                j += 1\n",
    "\n",
    "            followingSlopes[i] = s\n",
    "            sameSlopeDir[i] = int(j - 1)\n",
    "\n",
    "        # Calculate mean of local slopes\n",
    "        n = 20\n",
    "        means = np.zeros(len(slopes))\n",
    "        \n",
    "        for i in range(0, n):\n",
    "            means[i] = slopes[i]\n",
    "\n",
    "        for i in range(n, len(slopes)):\n",
    "            s = 0\n",
    "            for x in range(i-n, i+1):\n",
    "                s += slopes[x]\n",
    "            means[i] = s / n\n",
    "\n",
    "        # Calculate standard deviation of local slopes\n",
    "        STD = np.zeros(len(slopes))\n",
    "\n",
    "        for i in range(n, len(slopes)):\n",
    "            s = 0\n",
    "            for x in range(i-n, i+1):\n",
    "                s += (slopes[x] - means[i])**2\n",
    "            STD[i] = math.sqrt(s/(n-1))\n",
    "            \n",
    "        # Apply some considerations to detect offsets\n",
    "        firstTime = True\n",
    "        onsets = []\n",
    "        i = 0\n",
    "        while i < len(slopes):\n",
    "            i = int(i)\n",
    "            threshold = means[i] + STD[i]*t\n",
    "            if slopes[i] > threshold:\n",
    "                j = sameSlopeDir[i]\n",
    "                onsets.append(i+j)\n",
    "                i = i+j+1\n",
    "            else:\n",
    "                i += 1\n",
    "        \n",
    "        return onsets\n",
    "                    \n",
    "    \n",
    "    def __priorProbabilities(self,\n",
    "                           audio: np.array,\n",
    "                           frameLength: int,\n",
    "                           hopLength: int,\n",
    "                           pitchAcc: float = 0.9,\n",
    "                           voicedAcc: float = 0.9,\n",
    "                           onsetAcc: float = 0.9,\n",
    "                           spread: float = 0.2) -> np.array:\n",
    "        # Some constants\n",
    "        fMin = librosa.note_to_hz(self.noteMapValues[0])\n",
    "        fMax = librosa.note_to_hz(self.noteMapValues[-1])\n",
    "        nNotes = self.midiMax - self.midiMin + 1\n",
    "\n",
    "        # Extract pitch from audio\n",
    "        '''pitch, periodicity = torchcrepe.predict(audio,\n",
    "                           Fs,\n",
    "                           hop_length=hopLength,\n",
    "                           fmin=fMin,\n",
    "                           fmax=fMax,\n",
    "                           model='tiny',\n",
    "                           batch_size=frameLength,\n",
    "                           device='cuda:0',\n",
    "                           return_periodicity=True)\n",
    "        periodicity = torchcrepe.threshold.Silence(-40.)(periodicity,\n",
    "                                                 audio,\n",
    "                                                 Fs,\n",
    "                                                 hop_length=hopLength)\n",
    "\n",
    "        pitch = pitch.detach().cpu().numpy()[0]\n",
    "        periodicity = periodicity.detach().cpu().numpy()[0]'''\n",
    "        \n",
    "        pitch, voiced, _ = librosa.pyin(y=audio,\n",
    "                                     fmin=fMin*0.9,\n",
    "                                     fmax=fMax*1.1,\n",
    "                                     frame_length=frameLength,\n",
    "                                     win_length=int(frameLength / 2),\n",
    "                                     hop_length=hopLength\n",
    "                                    )\n",
    "        tuning = librosa.pitch_tuning(pitch)\n",
    "        f0_ = np.round(librosa.hz_to_midi(pitch - tuning)).astype(int)\n",
    "\n",
    "        # Calculate onsets positions\n",
    "        #onsets = librosa.onset.onset_detect(y=audio, sr=Fs, hop_length=hopLength, wait=1, pre_avg=1, post_avg=1, pre_max=1, post_max=1)\n",
    "        onsets = self.__detectVocalOnsets(pitch)\n",
    "        self.originalPitch = pitch\n",
    "        \n",
    "        # Init priors matrix\n",
    "        priors = np.ones((nNotes * 2 + 1, len(pitch)))\n",
    "\n",
    "        for nFrame in range(len(pitch)):\n",
    "            if (nFrame < len(voiced) and not voiced[nFrame]) or nFrame > len(voiced):\n",
    "                priors[0, nFrame] = voicedAcc\n",
    "            else:\n",
    "                priors[0, nFrame] = 1 - voicedAcc\n",
    "\n",
    "            for j in range(nNotes):\n",
    "                if nFrame in onsets:\n",
    "                    priors[(j * 2) + 1, nFrame] = onsetAcc\n",
    "                else:\n",
    "                    priors[(j * 2) + 1, nFrame] = 1 - onsetAcc\n",
    "\n",
    "                if j + self.midiMin == f0_[nFrame]:\n",
    "                    priors[(j * 2) + 2, nFrame] = pitchAcc\n",
    "                elif np.abs(j + self.midiMin - f0_[nFrame]) == 1:\n",
    "                    priors[(j * 2) + 2, nFrame] = pitchAcc * spread\n",
    "                else:\n",
    "                    priors[(j * 2) + 2, nFrame] = 1 - pitchAcc\n",
    "        \n",
    "        return priors\n",
    "        \n",
    "    def __statesToPianoroll(self, audio: np.array, states: list, frameLength: float, hopLength:float, hopTime: float) -> (list, list):\n",
    "        states_ = np.hstack((states, np.zeros(1)))\n",
    "\n",
    "        # Possible states\n",
    "        silence = 0\n",
    "        onset = 1\n",
    "        sustain = 2\n",
    "        \n",
    "        currentState = silence\n",
    "        output = []\n",
    "        melodyWave = []\n",
    "\n",
    "        lastOnset = 0\n",
    "        lastOffset = 0\n",
    "        lastMidi = 0\n",
    "\n",
    "        # Get RMS energy of the signal\n",
    "        rms = librosa.feature.rms(y=audio, frame_length=frameLength, hop_length=hopLength)\n",
    "        minRMS = min(rms[0])\n",
    "        maxRMS = max(rms[0])\n",
    "\n",
    "        currentRMSSum = 0\n",
    "        currentRMSNr = 0\n",
    "\n",
    "        for i, _ in tqdm(enumerate(states_)):\n",
    "            if currentState == silence:\n",
    "                # Onset found\n",
    "                if int(states_[i] % 2) != 0:\n",
    "                    lastOnset = i * hopTime\n",
    "                    lastMidi = ((states_[i] - 1) / 2) + self.midiMin\n",
    "                    lastNote = librosa.midi_to_note(lastMidi)\n",
    "                    currentState = onset\n",
    "\n",
    "                    melodyWave.append(self.noteMapHz[lastMidi] - self.originalPitch[i])\n",
    "\n",
    "                    currentRMSSum += rms[0][i]\n",
    "                    currentRMSNr += 1\n",
    "                else:\n",
    "                    melodyWave.append(0)\n",
    "            elif currentState == onset:\n",
    "                if int(states_[i] % 2) == 0:\n",
    "                    currentState = sustain\n",
    "\n",
    "                    melodyWave.append(self.noteMapHz[lastMidi] - self.originalPitch[i])\n",
    "\n",
    "                    currentRMSSum += rms[0][i]\n",
    "                    currentRMSNr += 1\n",
    "            elif currentState == sustain:\n",
    "                # Onset found\n",
    "                if int(states_[i] % 2) != 0:\n",
    "                    # Finish last note\n",
    "                    lastOffset = i * hopTime\n",
    "                    currentNote = [lastOnset, lastOffset, lastMidi, lastNote, \n",
    "                                   int( self.__rangeConversion(currentRMSSum/currentRMSNr, (minRMS, maxRMS), (0, 127)) )\n",
    "                                  ]\n",
    "                    output.append(currentNote)\n",
    "                    melodyWave.append(self.noteMapHz[lastMidi] - self.originalPitch[i])\n",
    "\n",
    "                    # Start new note\n",
    "                    lastOnset = i * hopTime\n",
    "                    lastMidi = ((states_[i] - 1) / 2) + self.midiMin\n",
    "                    lastNote = librosa.midi_to_note(lastMidi)\n",
    "                    currentState = onset\n",
    "\n",
    "                    currentRMSSum = rms[0][i]\n",
    "                    currentRMSNr = 1\n",
    "                elif states_[i] == 0:\n",
    "                    # Silence, end last note\n",
    "                    lastOffset = i * hopTime\n",
    "                    currentNote = [lastOnset, lastOffset, lastMidi, lastNote,\n",
    "                                   int( self.__rangeConversion(currentRMSSum/currentRMSNr, (minRMS, maxRMS), (0, 127)) )\n",
    "                                  ]\n",
    "                    output.append(currentNote)\n",
    "                    melodyWave.append(0)\n",
    "                    currentState = silence\n",
    "\n",
    "                    currentRMSSum = 0\n",
    "                    currentRMSNr = 0\n",
    "                else:\n",
    "                    melodyWave.append(self.noteMapHz[lastMidi] - self.originalPitch[i])\n",
    "        \n",
    "        melodyWave = np.nan_to_num(melodyWave).tolist()\n",
    "                    \n",
    "        return output, melodyWave\n",
    "\n",
    "    def __pianorollToMidi(self, bpm: float, pianoroll: list) -> midiutil.MidiFile:\n",
    "        quarterNote = 60 / bpm\n",
    "\n",
    "        onsets = np.array([p[0] for p in pianoroll])\n",
    "        offsets = np.array([p[1] for p in pianoroll])\n",
    "        velocities = np.array([p[4] for p in pianoroll])\n",
    "\n",
    "        onsets = onsets / quarterNote\n",
    "        offsets = offsets / quarterNote\n",
    "        durations = offsets - onsets\n",
    "\n",
    "        midi = midiutil.MIDIFile(1)\n",
    "        midi.addTempo(0, 0, bpm)\n",
    "\n",
    "        for i,_ in enumerate(onsets):\n",
    "            midi.addNote(0, 0, int(pianoroll[i][2]), onsets[i], durations[i], velocities[i])\n",
    "\n",
    "        return midi\n",
    "\n",
    "    def waveToMidi(self,\n",
    "                   audioPath: str,\n",
    "                   Fs: int = 22050,\n",
    "                   frameLength: int = 2048,\n",
    "                   hopLength: int = 512,\n",
    "                   pStayNote: float = 0.9,\n",
    "                   pStaySilence: float = 0.7,\n",
    "                   pitchAcc: float = 0.9,\n",
    "                   voicedAcc: float = 0.9,\n",
    "                   onsetAcc: float = 0.9,\n",
    "                   spread: float = 0.2\n",
    "                  ) -> (midiutil.MIDIFile(), list):\n",
    "\n",
    "        audio, Fs = librosa.load(audioPath)\n",
    "        transMat = self.__transitionMatrix(pStayNote, pStaySilence)\n",
    "        priors = self.__priorProbabilities(\n",
    "            audio,\n",
    "            frameLength,\n",
    "            hopLength,\n",
    "            pitchAcc,\n",
    "            voicedAcc,\n",
    "            onsetAcc,\n",
    "            spread\n",
    "        )\n",
    "\n",
    "        pInit = np.zeros(transMat.shape[0])\n",
    "        pInit[0] = 1\n",
    "        states = librosa.sequence.viterbi(priors, transMat, p_init=pInit)\n",
    "\n",
    "        pianoroll, melodyArray = self.__statesToPianoroll(audio,\n",
    "                                             states,\n",
    "                                             frameLength,\n",
    "                                             hopLength,\n",
    "                                             hopLength / Fs\n",
    "                                            )\n",
    "        bpm = 120 # To change\n",
    "        midi = self.__pianorollToMidi(bpm, pianoroll)\n",
    "\n",
    "        return midi, melodyArray\n",
    "\n",
    "\n",
    "ex = MidiExtractor()\n",
    "midi, melody = ex.waveToMidi(\"Adele.mp3\")\n",
    "with open(\"test.mid\", \"wb\") as outFile:\n",
    "    midi.writeFile(outFile)\n",
    "with open(\"test_melody.json\", \"w\") as outFile:\n",
    "    json.dump(melody, outFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb466b3-4359-4aad-916a-f1b3afa4d2e5",
   "metadata": {},
   "source": [
    "Instantiate the class and perform the extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f9d875e-b1d8-47e0-9e11-a098ec529855",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luigi\\AppData\\Local\\Temp\\ipykernel_27316\\2510561170.py:176: RuntimeWarning: invalid value encountered in cast\n",
      "  f0_ = np.round(librosa.hz_to_midi(pitch - tuning)).astype(int)\n",
      "C:\\Users\\Luigi\\AppData\\Local\\Temp\\ipykernel_27316\\2510561170.py:78: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  modF0[i] = (modF0[i] * threshold) / maximum\n",
      "C:\\Users\\Luigi\\AppData\\Local\\Temp\\ipykernel_27316\\2510561170.py:200: RuntimeWarning: overflow encountered in scalar subtract\n",
      "  elif np.abs(j + self.midiMin - f0_[nFrame]) == 1:\n",
      "10200it [00:00, 416324.29it/s]\n"
     ]
    }
   ],
   "source": [
    "ex = MidiExtractor()\n",
    "midi, melody = ex.waveToMidi(\"Adele.mp3\")\n",
    "with open(\"test.mid\", \"wb\") as outFile:\n",
    "    midi.writeFile(outFile)\n",
    "with open(\"test_melody.json\", \"w\") as outFile:\n",
    "    json.dump(melody, outFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0c2a6e-909b-4779-bc6b-731c54195fd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
